{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Monet CycleGAN\nThis project explores generating \"Monet Style\" images from photos implementing Cycle-Consistent Adversarial Network (CycleGAN). At the time of writing, this project is an assignment for \"DTSA 5511: Introduction to Deep Learning\". This is my first time implementing CycleGAN or working with TensorFlow Record (tfrec) files so I relied upon many sources and tutorials to complete this project. The main difference between this model from source documentation and tutorials is additional jittering steps and the implementation of an efficient neural network (ENet) as opposed to UNet or Resnet to construct the generator. I have included a comprehensive list of sources at the end of the notebook that I found helpful throughout the project.\n\nThis notebook can be accessed from my github repository here: \nhttps://github.com/arwhit/Monet-CycleGAN/tree/main\n\nYou can read more about the kaggle competition and access the original data files here:\nhttps://www.kaggle.com/competitions/gan-getting-started/overview\n","metadata":{}},{"cell_type":"code","source":"#Import neccesary libraries and packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Input, Add, Rescaling, Concatenate, ZeroPadding2D, Conv2D, MaxPool2D, Conv2DTranspose, UpSampling2D, Dropout, Flatten, GroupNormalization, BatchNormalization, Activation, LeakyReLU\nimport tensorflow as tf\nimport os\nimport time\nimport PIL\nimport shutil\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:32:14.056885Z","iopub.execute_input":"2023-08-13T15:32:14.057238Z","iopub.status.idle":"2023-08-13T15:32:14.063770Z","shell.execute_reply.started":"2023-08-13T15:32:14.057206Z","shell.execute_reply":"2023-08-13T15:32:14.062806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA and Preprocessing\n\nWe first need to access the images from the directory. In total, there are 300 monet images and 7028 photo images.","metadata":{}},{"cell_type":"code","source":"#Get all the file names\nmonet_files = tf.io.gfile.glob(str('/kaggle/input/gan-getting-started/monet_tfrec/*.tfrec'))\nphoto_files = tf.io.gfile.glob(str('/kaggle/input/gan-getting-started/photo_tfrec/*.tfrec'))\n","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:32:15.935996Z","iopub.execute_input":"2023-08-13T15:32:15.936783Z","iopub.status.idle":"2023-08-13T15:32:15.961685Z","shell.execute_reply.started":"2023-08-13T15:32:15.936747Z","shell.execute_reply":"2023-08-13T15:32:15.960768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we need to decode and read in the images. We will also normalize the images so that colors on the higher end of the RGB spectrum do not have a heavier impact on weights than colors on the lower end of the RGB spectrum. Lastly, we will apply some random jittering to introduce some variation in the dataset and prevent overfitting.There are mutliple ways to do this,but we will stick to taking a random crop of the original image and radomly flipping the image vertically or horizontally. In some scenarios it might be useful to also randomly adjust the hue, saturation, or contrast of the image, but that would not be appropriate here since these are features that often help distinguish an artists paintings.\n\nOne nice thing about TensorFlow is all of these functions can be wrapped into the data pipeline and applied to each batch with only a few lines of code.","metadata":{}},{"cell_type":"code","source":"#Implement helper functions to decode and preprocess the images\ndef preprocess_tfrecord(example,jitter=True):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    #decode\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    #normalize so colors on the upper end of the RGB spectrum are not washed out\n    image = (tf.cast(image, tf.float32)/ 127.5) - 1\n    #jitter\n    if jitter:\n        image=tf.image.resize(image, size=[286,286])\n        image=tf.image.random_crop(image, size=[256,256,3])\n        image=tf.image.random_flip_left_right(image)\n        image=tf.image.random_flip_up_down(image)\n        return image\n\n#implement helper function to load the datasets\ndef load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(preprocess_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n    return dataset\n\n#Create the data pipeline\nmonets = load_dataset(monet_files, labeled=True).shuffle(300).batch(1)\nphotos = load_dataset(photo_files, labeled=True).shuffle(300).batch(1)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:32:17.636042Z","iopub.execute_input":"2023-08-13T15:32:17.636418Z","iopub.status.idle":"2023-08-13T15:32:17.769489Z","shell.execute_reply.started":"2023-08-13T15:32:17.636386Z","shell.execute_reply":"2023-08-13T15:32:17.768501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have imported the data lets take a look at what a few images from each dataset look like.","metadata":{}},{"cell_type":"code","source":"#using next(iter()) is a convenient way to iterate through a dataset of TensorFlow object\nex_monet=next(iter(monets))\nex_photo=next(iter(photos))\n\n#Plot some examples\n#Create a subplot with 2 examples\nplt.subplot(121)\nplt.subplots_adjust(wspace=0.25)\nplt.title('Real Photo')\n#(note: * 0.5 + 0.5 operation is required to normalize the image so imshow can read it properly)\nplt.imshow(ex_photo[0] * 0.5 + 0.5)\nplt.subplot(122)\nplt.title('Monet')\nplt.imshow(ex_monet[0] * 0.5 + 0.5)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:32:26.165604Z","iopub.execute_input":"2023-08-13T15:32:26.165957Z","iopub.status.idle":"2023-08-13T15:32:27.899661Z","shell.execute_reply.started":"2023-08-13T15:32:26.165926Z","shell.execute_reply":"2023-08-13T15:32:27.898722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random jittering is wrapped into the model, but lets also use our examples to visualize what is happening.","metadata":{}},{"cell_type":"code","source":"#Example of jitter preprocessing\ndef jitter(image):\n    image=tf.image.resize(image, size=[286,286])\n    image=tf.image.random_crop(image, size=[256,256,3])\n    image=tf.image.random_flip_left_right(image)\n    image=tf.image.random_flip_up_down(image)\n    return image\n\nex_monet_new=jitter(ex_monet[0])\nex_photo_new=jitter(ex_photo[0])\n\n#Create a before and after subplot\nplt.subplot(221)\nplt.subplots_adjust(wspace=0.25, hspace=0.4)\nplt.title('Original')\n#(note: * 0.5 + 0.5 operation is required to normalize the image so imshow can read it properly)\nplt.imshow(ex_photo[0] * 0.5 + 0.5)\nplt.subplot(222)\nplt.title('Post Jitter')\n#(note: * 0.5 + 0.5 operation is required to normalize the image so imshow can read it properly)\nplt.imshow(ex_photo_new * 0.5 + 0.5)\nplt.subplot(223)\nplt.title('Original')\nplt.imshow(ex_monet[0] * 0.5 + 0.5)\nplt.subplot(224)\nplt.title('Post Jitter')\nplt.imshow(ex_monet_new * 0.5 + 0.5)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:32:39.320266Z","iopub.execute_input":"2023-08-13T15:32:39.320631Z","iopub.status.idle":"2023-08-13T15:32:40.174400Z","shell.execute_reply.started":"2023-08-13T15:32:39.320598Z","shell.execute_reply":"2023-08-13T15:32:40.173193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building\nNow we need to construct the actual model. One interesting feature of CycleGAN is that images from each class are unpaired, meaning we are picking up on general trends/styles and not linking two specific images together. This is not only more flexible than supervised approaches but also less costly as the need to identify and assign image pairs is unnecessary. \n\nThey also differ from vanilla GAN and DCGAN in a few ways, but the primary difference is the cyclical approach to evaluating the model. More specifically, the model does not simply measure the adversarial loss of the generated image, but the loss of the generated image translated back to its original class. This concept has been referred to as \"Cycle Consistency Loss\". To accomplish this the CycleGAN architecture has 2 discriminators and 2 generators that opperate in a cyclical manner, hence the name.The general cycle goes like this: \n\n1. an image from class a is passed through a generator to be transformed to an image of class b\n\n2. the class b discriminator makes a prediction if the image is of class b\n\n3. the transformed image is passed into another generator and transformed back to class a\n\n4. the difference between the original image and the regenerated image is measured and weights are updated\n\nNote: This is a gross oversimplification of the process, but should give you enough of an overview to interperet what is going on in the code below. If you need more details the full research paper that is linked in the sources section.\n\n","metadata":{}},{"cell_type":"markdown","source":"### Construct The Generators\nThere are a couple popular generator architechtures used for CycleGAN, but most papers/tutorial use some form of unet or resnet. Both of these methods are computationally expensive. For this project we will attempt to implement Enet which has been show to produce similar results in image segmentation tasks with smaller computational and memory requirements.","metadata":{}},{"cell_type":"code","source":"#Define Enccoder\ndef bn_encode(filters,  \n              ds=False,#case for downsampling\n              asym=False,#case for asymmetric convolution\n              dial=(1,1),#case for dialated convolution\n              drate=0.1,\n              dial_rate=1,\n              apply_norm=True):\n    \n    filters=int(filters)\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    result = Sequential()\n    \n    #Initial Projection\n    if ds:\n        result.add(Conv2D(filters, kernel_size=2, strides=2, padding='same',\n                          kernel_initializer=initializer, use_bias=False))\n    else:\n        result.add(Conv2D(filters/2, kernel_size=1, strides=1, padding='same',\n                          kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        result.add(BatchNormalization(gamma_initializer=gamma_init))\n        #result.add(GroupNormalization(groups=filters/2,gamma_initializer=gamma_init))\n        \n    result.add(LeakyReLU()) \n    #Main Convolution\n    if asym:\n        result.add(Conv2D(filters/2, kernel_size=(5,1), strides=1, padding='same',\n                          kernel_initializer=initializer, use_bias=False))\n        result.add(Conv2D(filters/2, kernel_size=(1,5), strides=1, padding='same',\n                          kernel_initializer=initializer, use_bias=False))\n    else:\n        result.add(Conv2D(filters/2, kernel_size=3, strides=1, padding='same',\n                          kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        result.add(BatchNormalization(gamma_initializer=gamma_init))\n        #result.add(GroupNormalization(groups=filters/2,gamma_initializer=gamma_init))\n        \n    result.add(LeakyReLU())\n    \n    #Final Expansion\n    result.add(Conv2D(filters, kernel_size=1, strides=1, padding='same',\n                      kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        result.add(BatchNormalization(gamma_initializer=gamma_init))\n        #result.add(GroupNormalization(groups=filters, gamma_initializer=gamma_init))\n    result.add(LeakyReLU())\n    \n    #Regulizer\n    result.add(Dropout(rate=drate))\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:32:51.878943Z","iopub.execute_input":"2023-08-13T15:32:51.879315Z","iopub.status.idle":"2023-08-13T15:32:51.893098Z","shell.execute_reply.started":"2023-08-13T15:32:51.879276Z","shell.execute_reply":"2023-08-13T15:32:51.892105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define Bottleneck Decoder\ndef bn_decode(filters,  \n              us=False,#case for upsampling\n              drate=0.1,\n              apply_norm=True):\n    \n    filters=int(filters)\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    result = Sequential()\n    \n    #Initial Projection\n    if us:\n        result.add(Conv2DTranspose(filters, kernel_size=2, strides=2, padding='same',\n                          kernel_initializer=initializer, use_bias=False))\n    else:\n        result.add(Conv2D(filters/2, kernel_size=1, strides=1, padding='same',\n                          kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        result.add(BatchNormalization(gamma_initializer=gamma_init))\n        #result.add(GroupNormalization(groups=filters/2,gamma_initializer=gamma_init))\n        \n    result.add(LeakyReLU()) \n    #Main Convolution\n    result.add(Conv2D(filters/2, kernel_size=3, strides=1,padding='same',\n                      kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        result.add(BatchNormalization(gamma_initializer=gamma_init))\n        #result.add(GroupNormalization(groups=filters/2,gamma_initializer=gamma_init))\n        \n    result.add(LeakyReLU())\n    \n    #Final Expansion\n    result.add(Conv2D(filters, kernel_size=1, strides=1, padding='same',\n                      kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        result.add(BatchNormalization(gamma_initializer=gamma_init))\n        #result.add(GroupNormalization(groups=filters, gamma_initializer=gamma_init))\n    result.add(LeakyReLU())\n    \n    #Regulizer\n    result.add(Dropout(rate=drate))\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:32:52.519822Z","iopub.execute_input":"2023-08-13T15:32:52.520704Z","iopub.status.idle":"2023-08-13T15:32:52.532408Z","shell.execute_reply.started":"2023-08-13T15:32:52.520658Z","shell.execute_reply":"2023-08-13T15:32:52.531293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it is time to create the generator following the architechture described in the Enet for image segmentation paper","metadata":{}},{"cell_type":"code","source":"#Define Generator\ndef Generator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    inputs = Input(shape=[256,256,3], batch_size=1)\n    \n    #initial Enet block\n    #output 16x128x128\n    I1=Conv2D(13,kernel_size=3,strides=2, padding='same',  \n              kernel_initializer=initializer, use_bias=False)(inputs)\n    I2=MaxPool2D(pool_size=(2, 2))(inputs)\n    I=Concatenate()([I1, I2])\n    #stage 1 (outputs 64x64x64)\n    #b1\n    b1=bn_encode(filters=64,ds=True,drate=0.01)(I)\n    paddings = [(0, 0), (0,0), (0, 0), (48, 0)]\n    I_padded = tf.pad(I, paddings, mode=\"constant\")\n    b1f=Add()([MaxPool2D(pool_size=(2, 2))(I_padded),b1])\n    #b11\n    b11=bn_encode(filters=64,drate=0.01)(b1f)\n    b11f=Add()([b1f,b11])\n    #b12\n    b12=bn_encode(filters=64,drate=0.01)(b11f)\n    b12f=Add()([b11f,b12])\n    #b13\n    b13=bn_encode(filters=64,drate=0.01)(b12f)\n    b13f=Add()([b12f,b13])\n    #b14\n    b14=bn_encode(filters=64,drate=0.01)(b13f)\n    b14f=Add()([b13f,b14])\n    \n    #stage 2 (ouputs 128x32x32)\n    #b2\n    b2=bn_encode(filters=128,ds=True)(b14f)\n    paddings = [(0, 0), (0,0), (0, 0), (64, 0)]\n    b14f_padded = tf.pad(b14f, paddings, mode=\"constant\")\n    b2f=Add()([MaxPool2D(pool_size=(2, 2))(b14f_padded),b2])\n    #b21\n    b21=bn_encode(filters=128)(b2f)\n    b21f=Add()([b2f,b21])    \n    #b22\n    b22=bn_encode(filters=128, dial_rate=(2,2))(b21f)\n    b22f=Add()([b21f,b22])    \n    #b23\n    b23=bn_encode(filters=128, asym=True)(b22f)\n    b23f=Add()([b22f,b23])       \n    #b24\n    b24=bn_encode(filters=128, dial_rate=(4,4))(b23f)\n    b24f=Add()([b23f,b24])    \n    #b25\n    b25=bn_encode(filters=128)(b24f)\n    b25f=Add()([b24f,b25])  \n    #b26\n    b26=bn_encode(filters=128, dial_rate=(8,8))(b25f)\n    b26f=Add()([b25f,b26])  \n    #b27\n    b27=bn_encode(filters=128, asym=True)(b26f)\n    b27f=Add()([b26f,b27]) \n    #b28\n    b28=bn_encode(filters=128, dial_rate=(16,16))(b27f)\n    b28f=Add()([b27f,b28])    \n    \n    #stage 3 (ouputs 128x32x32)\n    #b31\n    b31=bn_encode(filters=128)(b28f)\n    b31f=Add()([b28f,b31])    \n    #b32\n    b32=bn_encode(filters=128, dial_rate=(2,2))(b31f)\n    b32f=Add()([b31f,b32])    \n    #b33\n    b33=bn_encode(filters=128, asym=True)(b32f)\n    b33f=Add()([b32f,b33])       \n    #b34\n    b34=bn_encode(filters=128, dial_rate=(4,4))(b33f)\n    b34f=Add()([b33f,b34])    \n    #b35\n    b35=bn_encode(filters=128)(b34f)\n    b35f=Add()([b34f,b35])  \n    #b36\n    b36=bn_encode(filters=128, dial_rate=(8,8))(b35f)\n    b36f=Add()([b35f,b36])  \n    #b37\n    b37=bn_encode(filters=128, asym=True)(b36f)\n    b37f=Add()([b36f,b37]) \n    #b38\n    b38=bn_encode(filters=128, dial_rate=(16,16))(b37f)\n    b38f=Add()([b37f,b38])\n    \n    #stage 4 (outpust 64x64x64)\n    #b4\n    b4=bn_decode(filters=64,us=True)(b38f)\n    #b41\n    b41=bn_decode(filters=64)(b4)\n    b41f=Add()([b4,b41])\n    #b42\n    b42=bn_decode(filters=64)(b41f)\n    b42f=Add()([b41f,b42])\n    #stage 5 (outputs16x128x128)\n    #b5\n    b5=bn_decode(filters=16,us=True)(b42f)\n    #b51\n    b51=bn_decode(filters=16)(b5)\n    b51f=Add()([b5,b51])\n    #Upscale to original image size\n    last = Conv2DTranspose(3, 4,strides=2, padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh') # (bs, 256, 256, 3)\n    x = last(b51f)\n\n    return keras.Model(inputs=inputs, outputs=x)\n\nsample_generator=Generator()\nsample_generator.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:32:53.108164Z","iopub.execute_input":"2023-08-13T15:32:53.108607Z","iopub.status.idle":"2023-08-13T15:32:55.885193Z","shell.execute_reply.started":"2023-08-13T15:32:53.108568Z","shell.execute_reply":"2023-08-13T15:32:55.884451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generator to transform images to photo style\ngenerator_p=Generator()\n#generator to transfrom images to monet style\ngenerator_m=Generator()","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:32:55.886682Z","iopub.execute_input":"2023-08-13T15:32:55.887040Z","iopub.status.idle":"2023-08-13T15:33:01.332524Z","shell.execute_reply.started":"2023-08-13T15:32:55.887008Z","shell.execute_reply":"2023-08-13T15:33:01.331550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Construct the Discriminators\nThe discriminators will have a PatchGAN architechture. Since considerable time was used implementing ENet when building the generator, we will not reinvent the wheel with the discriminator. We will implement the discriminator avaliable in the tensorflow_examples package. Since the package is not downloadable from the kaggle notebook, we will pull the helper functions from the pix2pix tutorial and make minimal adjustments to work with the packages that have already been imported. Documentation for the package can be found in the sources section, but here is a quote from the documentaion on how it works.\n\n* Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n* The shape of the output after the last layer is (batch_size, 30, 30, 1).\n* Each 30 x 30 image patch of the output classifies a 70 x 70 portion of the input image.\n* The discriminator receives 2 inputs:\n     * The input image and the target image, which it should classify as real.\n     * The input image and the generated image (the output of the generator), which it     should classify as fake.\n * The 2 imputs are concatenated together an evaluated","metadata":{}},{"cell_type":"code","source":"#Implement downsample and discriminator helper functions from\n#https://www.tensorflow.org/tutorials/generative/pix2pix\n\ndef downsample(filters, size, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = Sequential()\n    result.add(Conv2D(filters, size, strides=2, padding='same',\n                      kernel_initializer=initializer, use_bias=False))\n    if apply_batchnorm:\n        result.add(BatchNormalization())\n    result.add(LeakyReLU())\n    return result\n\ndef Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    inp = Input(shape=[256, 256, 3], name='input_image',batch_size=1)\n    down1 = downsample(64, 4, False)(inp)  # (batch_size, 128, 128, 64)\n    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n    zero_pad1 = ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n    conv = Conv2D(512, 4, strides=1,\n                  kernel_initializer=initializer,\n                  use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n    batchnorm1 = BatchNormalization()(conv)\n    leaky_relu = LeakyReLU()(batchnorm1)\n    zero_pad2 = ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n    last = Conv2D(1, 4, strides=1,\n                  kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n    return keras.Model(inputs=inp, outputs=last)\n\nsample_discriminator=Discriminator()\nsample_discriminator.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:33:01.334437Z","iopub.execute_input":"2023-08-13T15:33:01.334799Z","iopub.status.idle":"2023-08-13T15:33:01.505190Z","shell.execute_reply.started":"2023-08-13T15:33:01.334765Z","shell.execute_reply":"2023-08-13T15:33:01.504399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#discriminator for photo style images\ndiscriminator_p=Discriminator()\n#discriminator for monet style images\ndiscriminator_m=Discriminator()","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:33:01.506184Z","iopub.execute_input":"2023-08-13T15:33:01.506725Z","iopub.status.idle":"2023-08-13T15:33:01.751428Z","shell.execute_reply.started":"2023-08-13T15:33:01.506689Z","shell.execute_reply":"2023-08-13T15:33:01.750463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define the Loss Function\nGenerator Loss- How good is the generator at generating images that look real to the discriminator?\n\nDiscriminator Loss- How good is the discriminator at determining if an image is generated?\n\nCycle Consistency Loss-How similar is an image that is tranformed to the different style and then transformed back to the original image\n\nIdentity Loss-How similar is a generated image of the same style (ex. similarity of a monet photo passed into the monet generator)","metadata":{}},{"cell_type":"code","source":"LAMBDA = 10\nloss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n#Discriminator Loss\ndef discriminator_loss(real, generated):\n    real_loss = loss_obj(tf.ones_like(real), real)\n    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n    total_disc_loss = real_loss + generated_loss\n    return total_disc_loss * 0.5\n#Generator Loss\ndef generator_loss(generated):\n    return loss_obj(tf.ones_like(generated), generated)\n#Cycle Consistency Loss\ndef calc_cycle_loss(real_image, cycled_image):\n    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n    return LAMBDA * loss1\n#Identity Loss\ndef identity_loss(real_image, same_image):\n    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n    return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:33:01.753316Z","iopub.execute_input":"2023-08-13T15:33:01.753741Z","iopub.status.idle":"2023-08-13T15:33:02.143560Z","shell.execute_reply.started":"2023-08-13T15:33:01.753709Z","shell.execute_reply":"2023-08-13T15:33:02.142472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define optimizers\ngenerator_m_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_p_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndiscriminator_m_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_p_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:33:02.895133Z","iopub.execute_input":"2023-08-13T15:33:02.895799Z","iopub.status.idle":"2023-08-13T15:33:02.911217Z","shell.execute_reply.started":"2023-08-13T15:33:02.895765Z","shell.execute_reply":"2023-08-13T15:33:02.910309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Link everything together into the CycleGAN model\n","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(real_photo, real_monet):\n  # persistent is set to True because the tape is used more than\n  # once to calculate the gradients.\n    with tf.GradientTape(persistent=True) as tape:\n        # Generator m translates to monet style\n        # Generator p translates to photo style\n        fake_monet = generator_m(real_photo, training=True)\n        cycled_photo = generator_p(fake_monet, training=True)\n\n        fake_photo = generator_p(real_monet, training=True)\n        cycled_monet = generator_m(fake_photo, training=True)\n\n        # same_photo and same_monet are used for identity loss.\n        same_photo = generator_p(real_photo, training=True)\n        same_monet = generator_m(real_monet, training=True)\n\n        disc_real_monet = discriminator_m(real_monet, training=True)\n        disc_real_photo = discriminator_p(real_photo, training=True)\n\n        disc_fake_monet = discriminator_m(fake_monet, training=True)\n        disc_fake_photo = discriminator_p(fake_photo, training=True)\n\n        # calculate the loss\n        gen_monet_loss = generator_loss(disc_fake_monet)\n        gen_photo_loss = generator_loss(disc_fake_photo)\n\n        total_cycle_loss = calc_cycle_loss(real_monet, cycled_monet) + calc_cycle_loss(real_photo, cycled_photo)\n\n        # Total generator loss = adversarial loss + cycle loss\n        total_gen_monet_loss = gen_monet_loss + total_cycle_loss + identity_loss(real_monet, same_monet)\n        total_gen_photo_loss = gen_photo_loss + total_cycle_loss + identity_loss(real_photo, same_photo)\n\n        disc_monet_loss = discriminator_loss(disc_real_monet, disc_fake_monet)\n        disc_photo_loss = discriminator_loss(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        generator_m_gradients = tape.gradient(total_gen_monet_loss, \n                                          generator_m.trainable_variables)\n        generator_p_gradients = tape.gradient(total_gen_photo_loss, \n                                          generator_p.trainable_variables)\n\n        discriminator_m_gradients = tape.gradient(disc_monet_loss, \n                                              discriminator_m.trainable_variables)\n        discriminator_p_gradients = tape.gradient(disc_photo_loss, \n                                              discriminator_p.trainable_variables)\n\n        # Apply the gradients to the optimizer\n    generator_m_optimizer.apply_gradients(zip(generator_m_gradients, \n                                            generator_m.trainable_variables))\n\n    generator_p_optimizer.apply_gradients(zip(generator_p_gradients, \n                                            generator_p.trainable_variables))\n\n    discriminator_m_optimizer.apply_gradients(zip(discriminator_m_gradients,\n                                                discriminator_m.trainable_variables))\n\n    discriminator_p_optimizer.apply_gradients(zip(discriminator_p_gradients,\n                                                discriminator_p.trainable_variables))","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:33:05.115912Z","iopub.execute_input":"2023-08-13T15:33:05.116290Z","iopub.status.idle":"2023-08-13T15:33:05.129503Z","shell.execute_reply.started":"2023-08-13T15:33:05.116237Z","shell.execute_reply":"2023-08-13T15:33:05.128423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results and Analysis\nNow that we have built the model and defined the loss functions, we need to train the model. I am only using free computational resources to train the model, so the number of epochs trained will be somewhat limited.","metadata":{}},{"cell_type":"markdown","source":"### Train the Model","metadata":{}},{"cell_type":"code","source":"for epoch in range(250):\n    start = time.time()\n    n = 0\n    for real_photo, real_monet in tf.data.Dataset.zip((photos, monets)):\n        train_step(real_photo, real_monet)\n        if n % 30 == 0:\n            print ('.', end='')\n        n += 1\n    print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time()-start))\n    if epoch % 10==0:    \n        prediction = generator_m(ex_photo_new[None, ...])\n        plt.figure(figsize=(6,6))\n        display_list = [ex_photo_new, prediction[0]]\n        title = ['Input Image', 'Predicted Image']\n\n        for i in range(2):\n            plt.subplot(1, 2, i+1)\n            plt.title(title[i])\n            # getting the pixel values between [0, 1] to plot it.\n            plt.imshow(display_list[i] * 0.5 + 0.5)\n            plt.axis('off')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-13T15:33:10.455140Z","iopub.execute_input":"2023-08-13T15:33:10.455868Z","iopub.status.idle":"2023-08-13T19:07:14.786033Z","shell.execute_reply.started":"2023-08-13T15:33:10.455829Z","shell.execute_reply":"2023-08-13T19:07:14.785082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test the model\nLets feed the model some samples to visualize how it performs. Since our original dataset applied random jittering, we will create a different dataset for visualization and submission where no jittering is applied.","metadata":{}},{"cell_type":"code","source":"#Implement helper functions to minimally process the images\ndef minprocess_tfrecord(example,jitter=True):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    #decode\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    #normalize so colors on the upper end of the RGB spectrum are not washed out\n    image = (tf.cast(image, tf.float32)/ 127.5) - 1\n    return image\n\n#implement helper function to reload the datasets\ndef reload_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(minprocess_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n    return dataset\n\nreloaded_photos=reload_dataset(photo_files)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:07:14.788109Z","iopub.execute_input":"2023-08-13T19:07:14.788486Z","iopub.status.idle":"2023-08-13T19:07:14.903294Z","shell.execute_reply.started":"2023-08-13T19:07:14.788449Z","shell.execute_reply":"2023-08-13T19:07:14.902312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_images(model, test_input):\n    prediction = model(test_input[None, ...])\n    plt.figure(figsize=(6, 6))\n    display_list = [test_input, prediction[0]]\n    title = ['Input Image', 'Predicted Image']\n\n    for i in range(2):\n        plt.subplot(1, 2, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()\n    \nfor inp in reloaded_photos.take(10):\n    generate_images(generator_m, inp)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:08:04.696794Z","iopub.execute_input":"2023-08-13T19:08:04.697206Z","iopub.status.idle":"2023-08-13T19:08:09.192381Z","shell.execute_reply.started":"2023-08-13T19:08:04.697172Z","shell.execute_reply":"2023-08-13T19:08:09.191480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears the model is transforming the images, but the results leave a lot to be desired. Photos will be generated and submitted to kaggle for a formal evaluation.","metadata":{}},{"cell_type":"code","source":"! mkdir ../images\ni = 1\nfor img in reloaded_photos:\n    prediction = generator_m(img[None,...])[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"../images/\" + str(i) + \".jpg\")\n    i += 1\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:27:28.828617Z","iopub.execute_input":"2023-08-13T19:27:28.828985Z","iopub.status.idle":"2023-08-13T19:46:55.264399Z","shell.execute_reply.started":"2023-08-13T19:27:28.828954Z","shell.execute_reply":"2023-08-13T19:46:55.262116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final score achieved for the project was","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\nWhile the model is opperational, the output is less than desirable. This might suggest that Enet architechture is not as well suited for CycleGAN as Unet or Resnet, but further testing would be required. If I were to develop the project further, I would try implememnting an Enet discriminator to see if using a more similiar discriminator effected the overall quality of the image produced. I would also test a hybrid Enet/Unet approach to see if adding skip connections to encoding and decoding layers of the same size helped produce more visually pleasing results. Additionally, I would also store the loss values at every step to to better diagnose the performance at each epoch. Lastly, I would try to figure out a workaround to achieve more epochs while still using a free cloud environment. This could probably be accomplished by exporting the weights and using them as a starting point to intitialize a new model.","metadata":{}},{"cell_type":"markdown","source":"### Sources\nhttps://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook\n\nhttps://www.tensorflow.org/tutorials/generative/dcgan\n\nhttps://www.tensorflow.org/tutorials/generative/pix2pix\n\nhttps://www.tensorflow.org/tutorials/generative/cyclegan\n\nhttps://arxiv.org/pdf/1606.02147.pdf\n\nhttps://arxiv.org/pdf/1703.10593.pdf\n\nhttps://arxiv.org/pdf/1611.07004.pdf\n\nhttps://arxiv.org/pdf/1512.03385.pdf\n\nhttps://aditi-mittal.medium.com/introduction-to-u-net-and-res-net-for-image-segmentation-9afcb432ee2f\n\nhttps://arxiv.org/pdf/1909.06840.pdf","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}